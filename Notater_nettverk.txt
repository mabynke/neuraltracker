De følgende forsøkene er kjørt med trening som automatisk slutter å trene når 5 epoker har gått uten forbedring i loss.

2017-07-12 - Forsøk:
Beskrivelse: Forsøk med data (1000 trening, 1000 test) med 1 4*4-kvadrat i tilfeldig bevegelse med tilfeldig startpunkt (32*32 piksler, hvit figur på svart bakgrunn). Grensesnittvektorstørrelse 512. Tilstandsvektorstørrelse 256. Ingen konvolusjonale lag. Optimeringsalgoritme: rmsprop
Resultat: Rundt 9 i loss (mean squared error) på testsettet, akseptabel følging av målet.

2017-07-13 - Forsøk:
Beskrivelse: Data med 2 like store hvite kvadrater som beveger seg uavhengig av hverandre.
Resultat: Dårlig. Mulig å få til overfitting ved lite treningssett, men testsettet mister fort målet og blir stående på samme sted.

2017-07-13 - Forsøk:
Beskrivelse: Endringer siden sist: Konvolusjonale lag: 3 konv-lag (32 filtre) med max-pooling (skaleringsfaktor 2) mellom.
Resultat: 0.64 loss for treningsdata, 37 for testdata. Overfitting, dårlige resultater for testsettet.

2017-07-13 - Forsøk:
Beskrivelse: Økt til 10000 treningssekvenser.
Resultat: Ikke overfitting, men 42 loss for testdata (39 for treningsdata). Ikke fungerende.

2017-07-13 - Forsøk:
Beskrivelse: Økt tilstandsvektorens lengde fra 256 til 512.
Resultat: Treningsloss: 3.7, testloss: 18.0. Overtilpasset, men ganske fungerende, men mister av og til målet og begynner å spore feil firkant eller virrer i et svart område. Blir spesielt forvirret når de to firkantene er nær hverandre.

2017-07-14 - Forsøk:
Beskrivelse: Firkantene bytter til en tilfeldig farge for hvert bilde.
Resultat: Treningsloss: 3.5, testloss: 24. Overtilpasset. Gjør det ikke veldig merkbart dårligere enn forrige forsøk selv om nettverket nå ikke kan se etter en bestemt farge (hvit). Ser fortsatt ofte på feil firkant.

2017-07-14 - Forsøk:
Beskrivelse: Firkantene har tilfeldig størrelse (men er alltid kvadratiske) mellom 3*3 og 10*10 (32//3). Bytter ikke lenger farge for hvert bilde, men hver firkant får en tilfeldig farge som den beholder gjennom sekvensen. De har også samme størrelse gjennom hele sekvensen.
Resultat: Treningsloss: 37, testloss: 36. Lite fungerende, men kan til en viss grad følge firkantene. Mister dem fort. Forventningen var at dette skulle være lettere å lære enn forrige forsøk. Gjentok forsøket med treningsloss 41, testloss 40.

2017-07-14 - Forsøk:
Beskrivelse: Økte antall filtre på det andre konvolusjonale laget fra 32 til 64 og på det tredje fra 32 til 128.
Resultat: Treningsloss: 50, testloss: 47. Treffer (fortsatt) ganske bra på første bilde, men sklir så mer eller mindre fort ut og mister firkanten viss den beveger seg raskt.

Forsøk:
Beskrivelse: Endret antall filtre i de tre konvolusjonale lagene fra 32, 64 og 128 til henholdsvis 128, 64 og 32.
Resultat: Treningsloss: 48, testloss: 47.

Forsøk:
Beskrivelse: Endret antall filtre tilbake til 32 i alle tre konvolusjonale lag. Endret lengden av grensesnittvektoren fra 512 til 1024.
Resultat: Trening: 45, test: 44.

Forsøk:
Beskrivelse: Deaktiverte den konvolusjonale delen og bruker kun et tett lag fra innbilder til grensesnittvektoren. Endret grensesnittvektoren tilbake fra 1024 til 512.
Resultat: Trening: 37, test: 39. Veldig lik oppførsel som tidligere.

Forsøk:
Beskrivelse: Økte lengen på tilstandrvektoren fra 512 til 1024. 7.9 mill parametre å trene (ca. dobbelt så mange som før).
Resultat: Trening: 34, test: 36. Ingen stor forbedring.

Forsøk01:
Beskrivelse: Endret tilstandsvektoren tilbake fra 1024 til 512. Bruker adagrad istedenfor rmsprop som optimeringsalgoritme.
Resultat: Treningen varte MYE lenger. Treningsloss: 9.9, test: 30.

Forsøk02:
Beskrivelse: Brukt et litt annen treningsmetode og "adagrad". Evaluerte med testsettet for hver 10. epoke.
Resultat: Nådde treningsloss 7.4, testloss 30.9. Testloss var underveis nede på 28,4, så overtilpassing ødela ikke veldig mye for testsettet. Klarer å følge den omtrentlige posisjonen til firkanten, men faller fort ut og er ikke særlig nøyaktig i posisjoneringen.

Forsøk:
Beskrivelse: Brukte optimeringsalgoritmen adam. Lagd egen rutine som slutter å trene når testloss ikke lenger blir bedre.
Resultat: Testloss 39.2, treningsloss 37.8. Treningslossen sluttet også å gå nedover. Forskjellen i testloss er ikke spesielt merkbar ved visuell inspeksjon av forutsigelser på testsettet. Velger ofte et punkt mellom de to firkantene, hvilket er fornuftig viss den ikke vet hvilken den skal følge, og vil minimere forventet loss. Den er god på å tilnærme størrelsen til den riktige firkanten.


Forsøk:
Beskrivelse: Endret representasjonen av plasseringen til firkanten på bildet fra (x_min, y_min, x_maks, y_maks) til (x, y, bredde, høyde) i merkelappene. Ikke endret nettverket. Formatet er x og y relativt til midten av bildet og avstanden fra midt til sidekant (x=1,y=1 er nederst til høyre), og bredde og høyde relativt til sidelengden på bildet (w=0.25, h=0.5 i et bilde på 32*32 har bredde 8 og høyde 16). De fire tallene er fortsatt samme utputt-tensor i modellen.
Resultat: Treningsloss 0.090, testloss 0.093. Tilnærmer fortsatt størrelsen på kvadratet bra, men er betraktelig dårligere på å finne plasseringen. Hopper mye mer enn før fra bilde til bilde.

Forsøk03:
Beskrivelse: Delt opp utputt i to vektorer (tette lag) av størrelse 2. Én (posisjonstensoren) representerer x og y, og den andre (størrelsestensoren) representerer w og h. Satte aktiveringen for størrelsestensoren til "sigmoid" og aktiveringen for posisjonstensoren til "linear". Kan nå sette vektingen for hver av tensorene i lossfunksjonen. Beholder foreløpig lik vekting ettersom loss for størrelsen blir mindre enn loss for posisjon med en faktor på 1000 eller 10000 og dermed uansett teller lite i summen. Loss for posisjonsvektoren (fortsatt mean square error) har nå også en direkte tolkning som kvadratet av den euklidske avstanden mellom fasit og forutsigelse.
Resultat: Ingen merkbar endring.

Forsøk:
Beskrivelse: Byttet ut LSTM-en med en tilsvarende GRU. Fortsatt 10 000 treningssekvenser.
Resultat: Testloss 0.142 (0.142 + 0.0000123), treningsloss 0.0052 (0.0052 + 0.0000114). Betraktelig bedre til å følge kvadratet. Mister av og til målet, spesielt i vanskelige tilfeller. Virrer en del rundt målet, men holder seg stort sett rundt det. Blir bare sjeldent betydelig forstyrret av den andre firkanten.

Forsøk04:
Beskrivelse: Økt antall testsekvenser til 100 000. Manuelt stoppet etter 6 epoker. (forsøk04b: stoppet etter 8 epoker)
Resultat: Testloss 0.0370 (0.0698 + 0.0000290), treningsloss 0.0316 (0.0316 + 0.0000331). Veldig bra resultat. God på posisjon.

Forsøk05:
Beskrivelse: Senket antall testsekvenser til 50 000. Stopper nå automatisk etter 6 epoker uten forbedring av testloss.
Resultat: testloss 0.048. Bra resultat.


Forsøk 2017-7-19-14-54:
Beskrivelse: Økt testsekvenser til 100 000. Lagt til enda et GRU-lag oppå det første. Tanken er at det skal gjøre det mulig med mer ikke-lineær prosessering for hvert tidssteg.
Resultat: Betydelig dårligere. Treningsloss etter 14 epoker: 0.149, testloss 0.149.

Forsøk 2017-7-20 9:23:38:
Beskrivelse: Økt til 10 000 testeksempler. Fjernet GRU-lag nr. 2 og byttet ut det eneste gjenværende GRU-laget med et LSTM-lag. Brukt 100 000 treningssekvenser.
Resultat: Samme som tidligere (med 10 000 treningssekvenser). Ca. 0.22 i loss for både trening og testing. Dvs. svært dårlig.

Forsøk 2017-07-20 14:16:11:
Beskrivelse: Kjøring med ett GRU-lag og 100 000 treningssekvenser, 10 000 testsekvenser.
Resultat: Samme som tidligere (på det beste (14 epoker) testloss 0.0323, treningsloss 0.0197).

Forsøk 2017-07-20 15:21:56:
Beskrivelse: Fjernet laget som konverterer dataene fra den konvolusjonale/maxpoolende delen til en grensesnittvektor. Med andre ord brukt tensoren direkte som grensesnittvektor. Parametre: 7 919 012.
Resultat: Konvergerer raskt. Ubetydelig forbedring etter 5 epoker. På det beste (39 epoker): testloss 0.0335, treningsloss 0.0121.

Forsøk 2017-07-20 17:52:29:
Beskrivelse: Utilsiktet gjentagelse av forrige forsøk.
Resultat: Svært likt forrige gang. På det beste (24 epoker) testloss 0.0363, treningsloss 0.0155.

Forsøk 2017-07-20 19:41:38:
Beskrivelse: Utilsiktet gjentagelse av forrige forsøk.
Resultat: Svært likt forrige gang. På det beste (21 epoker) testloss 0.0352, treningsloss 0.0149.

Forsøk 2017-07-21 12:15:04:
Beskrivelse: Gjeninnført grensesnittvektoren som før. Innført 0.5 dropout i GRU-laget (ikke i tidssteget).
Resultat: På det beste (14 epoker) testloss 0.0291, treningsloss 0.0298. Underveis gjennom læringen holder test- og treningsloss seg ganske like. Resultatene ser bra ut visuelt.

Forsøk 2017-07-21 14:35:24:
Beskrivelse: Innført enda et konvolusjonslag og et maxpoolinglag etter det det eksisterende maxpoolinglaget. Ikke brukt dropout (?).
Resultat: På det beste (23 epoker) testloss 0.0305, treningsloss 0.0179.

Forsøk 2017-07-21 16:23:31:
Beskrivelse: Innkoordinatene gis (etter å ha gått gjennom et tett lag) direkte som starttilstanden til GRU-laget og legges ikke på sekvensen som om de var det første bildet. GRU-en får (og gir ut) dermed 12 bilder lange sekvenser istedenfor 13.
Resultat: På det beste (21 epoker) testloss 0.0244, treningsloss 0.0252. Ikke overfitting (sannsynligvis pga. dropout). Loss holder seg under 0.029 over mange epoker, dvs. målbart bedre enn tidligere.

Forsøk 2017-07-21 17:54:39:
Skulle teste synkende læringsrate, men noe gikk sannsynligvis galt. Får loss rundt 0.23 – mye dårligere enn før.

Forsøk 2017-07-25 14:30:21:
Bildestørrelse 32*32. Usikker på hva som har blitt endret her utover at bildene blir "reskalert" fra 32*32 til 32*32.
Mye bedre enn tidligere. Beste (71 epoker): testloss  0.0168, treningsloss 0.0072

Forsøk 2017-07-31 12:25:12:
La til padding.
Enda mye bedre resultat: Beste (86 epoker): testloss 0.0117, treningsloss 0.0069. Ser visuelt veldig bra ut.

Forsøk 2017-07-31 13:19:14:
Kjører med tf_tracker sin make_model().
Overfitting, mangelen på dropout er sannsynlig årsak til at beste resultat (44 epoker) var testloss 0.022, treningsloss 0.0056.

Forsøk 2017-08-01 14:49:34:
128*128-bilder fra urbantracker.
Resultat: 

Forsøk 2017-08-01 15:22:27:
128*128-bilder fra urbantracker. Økt konvolusjon.
